import os
import pandas as pd
import numpy as np
import xgboost as xgb
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.metrics import r2_score, mean_absolute_error
from sklearn.preprocessing import StandardScaler
import matplotlib.pyplot as plt
from matplotlib import rcParams
import seaborn as sns
import joblib

# Configure font for Chinese characters on Windows systems
rcParams['font.sans-serif'] = ['SimHei']  # Use SimHei font
rcParams['axes.unicode_minus'] = False  # Fix minus sign display issue


# 1. Data preparation function
def load_data(folder_path, window_pct=0.1):
    """Load data and extract features from the first 10% time window"""
    features = []
    labels = []

    for file in [f for f in os.listdir(folder_path) if f.endswith('.csv')]:
        try:
            capacity = float(file.replace('.csv', ''))
            df = pd.read_csv(os.path.join(folder_path, file),
                             header=None,
                             names=['time', 'value'])

            window_size = max(1, int(len(df) * window_pct))
            window_data = df.iloc[:window_size]

            # Feature engineering
            value_diff = np.diff(window_data['value'])

            features.append({
                'initial': window_data['value'].iloc[0],
                'mean': window_data['value'].mean(),
                'std': window_data['value'].std(),
                'slope': np.polyfit(window_data['time'], window_data['value'], 1)[0],
                'max_diff': value_diff.max(),
                'min_diff': value_diff.min(),
                'abs_energy': np.sum(window_data['value'] ** 2),
                'q25': np.percentile(window_data['value'], 25),
                'q75': np.percentile(window_data['value'], 75),
                'entropy': np.log(np.var(window_data['value']) + 1e-8),
                'zero_cross': ((value_diff[:-1] * value_diff[1:]) < 0).sum(),
                'trend_strength': np.abs(np.polyfit(window_data['time'], window_data['value'], 1)[0]) /
                                  (window_data['value'].std() + 1e-8)
            })
            labels.append(capacity)
        except Exception as e:
            print(f"Error processing file {file}: {str(e)}")
            continue

    return pd.DataFrame(features), np.array(labels)


# 2. Load data (using first 10% time window)
data_path = r"C:\Users\Liuhongwei\Desktop\capacity-forecast"
X, y = load_data(data_path, window_pct=0.1)

# 3. Data preprocessing
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# 4. Split dataset into training and testing sets (80:20 ratio)
X_train, X_test, y_train, y_test = train_test_split(
    X_scaled, y, test_size=0.2, random_state=42)

# 5. XGBoost model training and tuning
print("\nStarting XGBoost training...")
xgb_model = xgb.XGBRegressor(objective='reg:squarederror',
                             random_state=42,
                             n_jobs=-1)

# Hyperparameter grid search
param_grid = {
    'n_estimators': [100, 200, 300],
    'max_depth': [3, 5, 7, 9],
    'learning_rate': [0.01, 0.05, 0.1],
    'subsample': [0.8, 0.9, 1.0],
    'colsample_bytree': [0.8, 0.9, 1.0],
    'gamma': [0, 0.1, 0.2]
}

grid_search = GridSearchCV(estimator=xgb_model,
                           param_grid=param_grid,
                           cv=5,
                           scoring='r2',
                           n_jobs=1,
                           verbose=2)
grid_search.fit(X_train, y_train)

# Get best model
best_xgb = grid_search.best_estimator_

# 6. Model evaluation
y_pred = best_xgb.predict(X_test)

r2 = r2_score(y_test, y_pred)
mae = mean_absolute_error(y_test, y_pred)

print("\n=== Model Evaluation Results ===")
print(f"Best parameters: {grid_search.best_params_}")
print(f"Test set RÂ²: {r2:.4f}")
print(f"Test set MAE: {mae:.4f}")

# 7. Feature importance analysis
plt.figure(figsize=(12, 8))
xgb.plot_importance(best_xgb, height=0.8)
plt.title("XGBoost Feature Importance", fontsize=16)
plt.xlabel("F Score", fontsize=14)
plt.ylabel("Feature Name", fontsize=14)
plt.tight_layout()
plt.savefig('xgb_feature_importance.png', dpi=300, bbox_inches='tight')
plt.close()

# 8. Save model
joblib.dump(best_xgb, 'optimized_xgb_model.pkl')
joblib.dump(scaler, 'scaler.pkl')
print("\nModel saved as optimized_xgb_model.pkl")

# 9. Prediction results visualization
plt.figure(figsize=(10, 8))
plt.scatter(y_test, y_pred, alpha=0.6, color='royalblue')
plt.plot([y.min(), y.max()], [y.min(), y.max()], 'r--', linewidth=2)
plt.xlabel("True Values", fontsize=14)
plt.ylabel("Predicted Values", fontsize=14)
plt.title("XGBoost Prediction Results", fontsize=16)
plt.grid(True, alpha=0.3)
plt.tight_layout()
plt.savefig('xgb_prediction_plot.png', dpi=300, bbox_inches='tight')
plt.close()

# 10. Residual analysis
residuals = y_test - y_pred
plt.figure(figsize=(10, 6))
sns.histplot(residuals, kde=True, color='teal', bins=20)
plt.xlabel("Residuals", fontsize=14)
plt.ylabel("Frequency", fontsize=14)
plt.title("XGBoost Residual Distribution", fontsize=16)
plt.grid(True, alpha=0.3)
plt.tight_layout()
plt.savefig('xgb_residuals_dist.png', dpi=300, bbox_inches='tight')
plt.close()

print("\nAll visualization plots have been saved as PNG files")
